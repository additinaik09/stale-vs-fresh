<!DOCTYPE html>
<!-- saved from url=(0052)https://www.akshaymakes.com/blogs/vision-transformer -->
<html lang="en" class="dark"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<link rel="icon" href="https://www.akshaymakes.com/favicon.png">
	<meta name="viewport" content="width=device-width">
	<meta name="google-site-verification" content="s-55Xg-C3a4I0iqmxy7h35Vwd2supesKQqyyCleOmPk">
	<meta name="author" content="Akshay Ballal">
	<meta name="description" content="Welcome to my personal website! I am a self-taught AI developer driven by a passion for pushing the boundaries of technology. Applying First Principles thinking, I strive to solve complex challenges and create innovative solutions. As a Technology Enthusiast, I constantly explore the latest advancements in the field. I am deeply committed to leveraging AI for social good and advocate for green technology. Join me on this journey as I utilize my self-taught expertise to build products and technologies that address industrial problems. Let&#39;s collaborate and shape a better future together.">

	<meta property="og:title" content="Akshay&#39;s Personal Website">
	<meta property="og:description" content="I am a Machine Learning Enthusiast. Check out my Projects and Blogs">
	<meta property="og:url" content="www.akshaymakes.com">
	<meta property="og:image" content="https://res.cloudinary.com/dltwftrgc/image/upload/t_Facebook ad/v1683659009/Blogs/AI_powered_game_bot/profile_lyql45.jpg">

	<meta name="robots" content="follow, index, max-snippet:-1, max-video-preview:-1, max-image-preview:large">
	<title>Building a Vision Transformer from Scratch in PyTorchðŸ”¥</title>

	<meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:creator" content="@akshayballal95">
	<meta property="twitter:image" content="https://res.cloudinary.com/dltwftrgc/image/upload/c_lfill,g_auto,h_1080,w_1080/v1683659009/Blogs/AI_powered_game_bot/profile_lyql45.jpg">
	<meta property="twitter:title" content="Akshay Ballal - Machine Learning Enthusiast">
	<meta property="twitter:description" content="I am a self-taught AI developer driven by a passion for pushing the boundaries of technology. Applying First Principles thinking, I strive to solve complex challenges and create innovative solutions. As a Technology Enthusiast, I constantly explore the latest advancements in the field. I am deeply committed to leveraging AI for social good and advocate for green technology. Join me on this journey as I utilize my self-taught expertise to build products and technologies that address industrial problems. Let&#39;s collaborate and shape a better future together.">

	
		<link href="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/index.218568fa.css" rel="stylesheet">
		<link href="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/ProgressBar.4f1e9ba5.css" rel="stylesheet"><title>Building a Vision Transformer from Scratch in PyTorchðŸ”¥</title><!-- HTML_TAG_START --><!-- HTML_TAG_END --><script nonce="%sveltekit.nonce%">(function b(){const e=document.documentElement.classList,t=localStorage.getItem("modeUserPrefers")==="false",s=!("modeUserPrefers"in localStorage),o=window.matchMedia("(prefers-color-scheme: dark)").matches;t||s&&o?e.add("dark"):e.remove("dark")})();</script><meta name="description" content="Demystifying the Architecture and Implementation Steps for State-of-the-Art Image Classification"><meta name="title" content="Building a Vision Transformer from Scratch in PyTorchðŸ”¥"><meta property="og:image" content="https://res.cloudinary.com/dltwftrgc/image/upload/v1687028785/Blogs/Vision-Transformer/cover_image_2_havvkp.jpg"><meta property="og:description" content="Demystifying the Architecture and Implementation Steps for State-of-the-Art Image Classification"><meta property="og:title" content="Building a Vision Transformer from Scratch in PyTorchðŸ”¥"><meta property="og:description" content="Demystifying the Architecture and Implementation Steps for State-of-the-Art Image Classification"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:creator" content="@akshayballal95"><meta property="twitter:image" content="https://res.cloudinary.com/dltwftrgc/image/upload/v1687028785/Blogs/Vision-Transformer/cover_image_2_havvkp.jpg"><meta property="twitter:title" content="Building a Vision Transformer from Scratch in PyTorchðŸ”¥"><meta property="twitter:description" content="Demystifying the Architecture and Implementation Steps for State-of-the-Art Image Classification"><meta name="robots" content="follow, index, max-snippet:-1, max-video-preview:-1, max-image-preview:large">
		<link href="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/fontawesome.min.css" rel="stylesheet">
		<link href="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/brands.min.css" rel="stylesheet">
		<link href="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/solid.min.css" rel="stylesheet">
	<link rel="stylesheet" href="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

<link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/nodes/0.56b3d91f.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/chunks/index.a8636d2b.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/nodes/1.304c39a4.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/chunks/singletons.5363c940.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/chunks/index.f069b694.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/chunks/paths.202d5093.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/nodes/2.226fa804.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/chunks/index.453e11c5.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/chunks/index.729673ef.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/chunks/stores.a68e2093.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/chunks/ProgressBar.svelte_svelte_type_style_lang.b403c969.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/chunks/Tab.2ebe9a7e.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/chunks/stateStore.ca5a164d.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/chunks/navigation.249c2f2f.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/nodes/3.56b3d91f.js"><link rel="modulepreload" as="script" crossorigin="" href="https://www.akshaymakes.com/_app/immutable/nodes/7.13179599.js"><script src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/script.js.download" defer="" data-sdkn="@vercel/analytics" data-sdkv="1.3.1"></script></head>

<body data-sveltekit-preload-data="hover" data-theme="skeleton">
	<div style="display: contents"><div id="appShell" class="w-full h-full flex flex-col overflow-hidden " data-testid="app-shell"><header id="shell-header" class="flex-none z-10"><div class="app-bar flex flex-col bg-surface-100-800-token  space-y-4 p-4  " data-testid="app-bar" role="toolbar" aria-label="" aria-labelledby=""><div class="app-bar-row-main grid items-center grid-cols-[auto_1fr_auto] gap-4 "><div class="app-bar-slot-lead flex-none flex justify-between items-center "><div class="lg:flex-row flex flex-col items-baseline whitespace-nowrap cursor-pointer"><p class="unstyled font-bold text-2xl">Akshay Ballal</p> <span class="flex"><p class="unstyled italic text-l hidden lg:block">/</p><p class="unstyled italic text-l">MACHINE LEARNING ENTHUSIAST</p></span></div></div> <div class="app-bar-slot-default flex-auto "></div> <div class="app-bar-slot-trail flex-none flex items-center space-x-4 "><div class="tab-group space-y-4 bg-surface-100-800-token w-full hidden md:block" data-testid="tab-group"><div class="tab-list flex overflow-x-auto hide-scrollbar justify-center gap-2  " role="tablist" aria-labelledby=""><label class="text-center cursor-pointer transition-colors duration-100 flex-1 lg:flex-none px-4 py-2  hover:variant-soft-primary rounded" title=""><div class="tab " data-testid="tab" role="tab" aria-controls="" aria-selected="false" tabindex="-1"><div class="h-0 w-0 overflow-hidden"><input type="radio" name="about_me" tabindex="-1" value="0"></div> <div class="tab-interface  space-y-1"> <div class="tab-label"><p class="font-light">About Me</p></div></div></div></label> <label class="text-center cursor-pointer transition-colors duration-100 flex-1 lg:flex-none px-4 py-2  hover:variant-soft-primary rounded" title=""><div class="tab " data-testid="tab" role="tab" aria-controls="" aria-selected="false" tabindex="-1"><div class="h-0 w-0 overflow-hidden"><input type="radio" name="resume" tabindex="-1" value="1"></div> <div class="tab-interface  space-y-1"> <div class="tab-label"><p class="font-light">Resume</p></div></div></div></label> <label class="text-center cursor-pointer transition-colors duration-100 flex-1 lg:flex-none px-4 py-2  hover:variant-soft-primary rounded" title=""><div class="tab " data-testid="tab" role="tab" aria-controls="" aria-selected="false" tabindex="-1"><div class="h-0 w-0 overflow-hidden"><input type="radio" name="projects" tabindex="-1" value="2"></div> <div class="tab-interface  space-y-1"> <div class="tab-label"><p class="font-light">Projects</p></div></div></div></label> <label class="text-center cursor-pointer transition-colors duration-100 flex-1 lg:flex-none px-4 py-2  hover:variant-soft-primary rounded" title=""><div class="tab " data-testid="tab" role="tab" aria-controls="" aria-selected="false" tabindex="-1"><div class="h-0 w-0 overflow-hidden"><input type="radio" name="Contact" tabindex="-1" value="3"></div> <div class="tab-interface  space-y-1"> <div class="tab-label"><p class="font-light">Contact</p></div></div></div></label> <label class="text-center cursor-pointer transition-colors duration-100 flex-1 lg:flex-none px-4 py-2  variant-filled-primary rounded" title=""><div class="tab " data-testid="tab" role="tab" aria-controls="" aria-selected="true" tabindex="0"><div class="h-0 w-0 overflow-hidden"><input type="radio" name="Blog" tabindex="-1" value="4"></div> <div class="tab-interface  space-y-1"> <div class="tab-label"><p class="font-light">Blog</p></div></div></div></label> <label class="text-center cursor-pointer transition-colors duration-100 flex-1 lg:flex-none px-4 py-2  hover:variant-soft-primary rounded" title=""><div class="tab " data-testid="tab" role="tab" aria-controls="" aria-selected="false" tabindex="-1"><div class="h-0 w-0 overflow-hidden"><input type="radio" name="Chat" tabindex="-1" value="5"></div> <div class="tab-interface  space-y-1"> <div class="tab-label"><p class="font-light">Chat</p></div></div></div></label></div> </div> <button class="btn-icon variant-filled md:hidden"><i class="fa-solid fa-arrow-down"></i></button>  <div class="lightswitch-track cursor-pointer transition-all duration-[200ms] w-12 h-6 ring-[1px] ring-surface-500/30 rounded-token bg-surface-900 " role="switch" aria-label="Light Switch" aria-checked="false" title="Toggle Light Mode" tabindex="0"><div class="lightswitch-thumb aspect-square scale-[0.8] flex justify-center items-center transition-all duration-[200ms] h-6 rounded-token bg-surface-50 "><svg class="lightswitch-icon w-[70%] aspect-square fill-surface-900" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M223.5 32C100 32 0 132.3 0 256S100 480 223.5 480c60.6 0 115.5-24.2 155.8-63.4c5-4.9 6.3-12.5 3.1-18.7s-10.1-9.7-17-8.5c-9.8 1.7-19.8 2.6-30.1 2.6c-96.9 0-175.5-78.8-175.5-176c0-65.8 36-123.1 89.3-153.3c6.1-3.5 9.2-10.5 7.7-17.3s-7.3-11.9-14.3-12.5c-6.3-.5-12.6-.8-19-.8z"></path></svg></div></div></div></div> </div></header> <div class="flex-auto w-full h-full flex overflow-hidden"> <div id="page" class=" flex-1 overflow-x-hidden flex flex-col"> <main id="page-content" class="flex-auto "> <div class="flex justify-center lg:p-10 p-5 mx-auto overflow-hidden"><div id="blog" class="flex flex-col gap-8 xl:w-1/2 md:w-3/4 w-full rounded-md card md:p-8 p-5 relative"><h2 class="mt-5">Building a Vision Transformer from Scratch in PyTorchðŸ”¥</h2> <p>19 Jun 2023</p> <p class="z-0"><img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/cover_image_bgxvpq.jpg" alt="https://res.cloudinary.com/dltwftrgc/image/upload/v1687028786/Blogs/Vision-Transformer/cover_image_bgxvpq.jpg" class="rounded-md"></p>
<h2>Introduction</h2>
<p class="z-0">In recent years, the field of computer vision has been revolutionized by the advent of transformer models. Originally designed for natural language processing tasks, transformers have proven to be incredibly powerful in capturing spatial dependencies in visual data as well. The Vision Transformer (ViT) is a prime example of this, presenting a novel architecture that achieves state-of-the-art performance on various image classification tasks.</p>
<p class="z-0">In this article, we will embark on a journey to build our very own Vision Transformer using PyTorch. By breaking down the implementation step by step, we aim to provide a comprehensive understanding of the ViT architecture and enable you to grasp its inner workings with clarity. Of course, we could always use the PyTorchâ€™s inbuilt implementation of the Vision Transformer Model, but whatâ€™s the fun in that. </p>
<p class="z-0">We will start by setting up the necessary dependencies and libraries, ensuring a smooth workflow throughout the project. Next, we will dive into data acquisition, where we obtain a suitable dataset to train our Vision Transformer model.</p>
<p class="z-0">To prepare the data for training, we will define the necessary transformations required for augmenting and normalizing the input images. With the data transformations in place, we will proceed to create a custom dataset and data loaders, setting the stage for training our model.</p>
<p class="z-0">For understanding the Vision Transformer architecture, it is crucial to build it from scratch. In the subsequent sections, we will dissect each component of the ViT model and explain its purpose. We will begin with the Patch Embedding Layer, responsible for dividing the input image into smaller patches and embedding them into a vector format. Following that, we will explore the Multi-Head Self Attention Block, which allows the model to capture global and local relationships within the patches.</p>
<p class="z-0">Additionally, we will delve into the Machine Learning Perceptron Block, a key component that enables the model to capture the hierarchical representations of the input data. By assembling these components, we will construct the Transformer Block, which forms the core building block of the Vision Transformer.</p>
<p class="z-0">Finally, we will bring it all together by creating the ViT model, utilizing the components we have meticulously crafted. With the completed model, we can experiment with it, fine-tune it, and unleash its potential on various computer vision tasks.</p>
<p class="z-0">By the end of this post, you will have gained a solid understanding of the Vision Transformer architecture and its implementation in PyTorch. Armed with this knowledge, you will be able to modify and extend the model to suit your specific needs or even build upon it for advanced computer vision applications. Letâ€™s start.</p>
<h2>Step 0: Install and Import Dependencies</h2>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #FDAEB7; font-style: italic">!</span><span style="color: #E1E4E8">pip install </span><span style="color: #F97583">-</span><span style="color: #E1E4E8">q torchinfo</span></span></code></pre>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">import</span><span style="color: #E1E4E8"> torch</span></span>
<span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> torch </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> nn</span></span>
<span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> torchinfo </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> summary</span></span>
<span class="line"></span></code></pre>
<hr>
<h2>Step 1: Get the Data</h2>
<p class="z-0">1.1. Download the Data from the Web (It will be a .zip file for this)
1.2. Extract the zip file
1.3. Delete the zip file</p>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">import</span><span style="color: #E1E4E8"> requests</span></span>
<span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> pathlib </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> Path</span></span>
<span class="line"><span style="color: #F97583">import</span><span style="color: #E1E4E8"> os</span></span>
<span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> zipfile </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> ZipFile</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Define the URL for the zip file</span></span>
<span class="line"><span style="color: #E1E4E8">url </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip"</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Send a GET request to download the file</span></span>
<span class="line"><span style="color: #E1E4E8">response </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> requests.get(url)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Define the path to the data directory</span></span>
<span class="line"><span style="color: #E1E4E8">data_path </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> Path(</span><span style="color: #9ECBFF">"data"</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Define the path to the image directory</span></span>
<span class="line"><span style="color: #E1E4E8">image_path </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> data_path </span><span style="color: #F97583">/</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"pizza_steak_sushi"</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Check if the image directory already exists</span></span>
<span class="line"><span style="color: #F97583">if</span><span style="color: #E1E4E8"> image_path.is_dir():</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">"</span><span style="color: #79B8FF">{</span><span style="color: #E1E4E8">image_path</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> directory exists."</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #F97583">else</span><span style="color: #E1E4E8">:</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">"Did not find </span><span style="color: #79B8FF">{</span><span style="color: #E1E4E8">image_path</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> directory, creating one..."</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #E1E4E8">    image_path.mkdir(</span><span style="color: #FFAB70">parents</span><span style="color: #F97583">=</span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">exist_ok</span><span style="color: #F97583">=</span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Write the downloaded content to a zip file</span></span>
<span class="line"><span style="color: #F97583">with</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">open</span><span style="color: #E1E4E8">(data_path </span><span style="color: #F97583">/</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"pizza_steak_sushi.zip"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"wb"</span><span style="color: #E1E4E8">) </span><span style="color: #F97583">as</span><span style="color: #E1E4E8"> f:</span></span>
<span class="line"><span style="color: #E1E4E8">    f.write(response.content)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Extract the contents of the zip file to the image directory</span></span>
<span class="line"><span style="color: #F97583">with</span><span style="color: #E1E4E8"> ZipFile(data_path </span><span style="color: #F97583">/</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"pizza_steak_sushi.zip"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"r"</span><span style="color: #E1E4E8">) </span><span style="color: #F97583">as</span><span style="color: #E1E4E8"> zipref:</span></span>
<span class="line"><span style="color: #E1E4E8">    zipref.extractall(image_path)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Remove the downloaded zip file</span></span>
<span class="line"><span style="color: #E1E4E8">os.remove(data_path </span><span style="color: #F97583">/</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">'pizza_steak_sushi.zip'</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<hr>
<h2>Step 2: Define Transformations</h2>
<ol class="list-outside"><li>Resize the images using <code>Resize()</code> to 224. We choose the images size to be 224 based on the ViT Paper</li>
<li>Convert to Tensor using <code>ToTensor()</code></li></ol>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> torchvision.transforms </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> Resize, Compose, ToTensor</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D">## Define the train_transform using Compose</span></span>
<span class="line"><span style="color: #E1E4E8">train_transform </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> Compose([</span></span>
<span class="line"><span style="color: #E1E4E8">    Resize((</span><span style="color: #79B8FF">224</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">224</span><span style="color: #E1E4E8">)),</span></span>
<span class="line"><span style="color: #E1E4E8">    ToTensor()</span></span>
<span class="line"><span style="color: #E1E4E8">])</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D">## Define the test_transform using Compose</span></span>
<span class="line"><span style="color: #E1E4E8">test_transform </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> Compose([</span></span>
<span class="line"><span style="color: #E1E4E8">    Resize((</span><span style="color: #79B8FF">224</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">224</span><span style="color: #E1E4E8">)),</span></span>
<span class="line"><span style="color: #E1E4E8">    ToTensor()</span></span>
<span class="line"><span style="color: #E1E4E8">])</span></span></code></pre>
<hr>
<h2>Step 3: Create Dataset and DataLoader</h2>
<p class="z-0">We can use PyTorchâ€™s ImageFolder DataSet library to create our Datasets.</p>
<p class="z-0">For ImageFolder to work this is how your data folder needs to be structured.</p>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">data</span></span>
<span class="line"><span style="color: #E1E4E8">â””â”€â”€ pizza_steak_sushi</span></span>
<span class="line"><span style="color: #E1E4E8">    â”œâ”€â”€ test</span></span>
<span class="line"><span style="color: #E1E4E8">    â”‚   â”œâ”€â”€ pizza</span></span>
<span class="line"><span style="color: #E1E4E8">    â”‚   â”œâ”€â”€ steak</span></span>
<span class="line"><span style="color: #E1E4E8">    â”‚   â””â”€â”€ sushi</span></span>
<span class="line"><span style="color: #E1E4E8">    â””â”€â”€ train</span></span>
<span class="line"><span style="color: #E1E4E8">        â”œâ”€â”€ pizza</span></span>
<span class="line"><span style="color: #E1E4E8">        â”œâ”€â”€ steak</span></span>
<span class="line"><span style="color: #E1E4E8">        â””â”€â”€ sushi</span></span></code></pre>
<p class="z-0">All the <code>pizza</code> images will be in the pizza folder of train and test sub folders and so on for all the classes that you have.</p>
<p class="z-0">There are two useful methods that you can call on the created <code>training_dataset</code> and <code>test_dataset</code></p>
<ol class="list-outside"><li><code>training_dataset.classes</code> that gives <code>['pizza', 'steak', 'sushi']</code></li>
<li><code>training_dataset.class_to_idx</code> that gives <code>{'pizza': 0, 'steak': 1, 'sushi': 2}</code></li></ol>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> torchvision.datasets </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> ImageFolder</span></span>
<span class="line"><span style="color: #F97583">from</span><span style="color: #E1E4E8"> torch.utils.data </span><span style="color: #F97583">import</span><span style="color: #E1E4E8"> DataLoader</span></span>
<span class="line"></span>
<span class="line"><span style="color: #79B8FF">BATCH_SIZE</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">32</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Define the data directory</span></span>
<span class="line"><span style="color: #E1E4E8">data_dir </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> Path(</span><span style="color: #9ECBFF">"data/pizza_steak_sushi"</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Create the training dataset using ImageFolder</span></span>
<span class="line"><span style="color: #E1E4E8">training_dataset </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> ImageFolder(</span><span style="color: #FFAB70">root</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">data_dir </span><span style="color: #F97583">/</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"train"</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">transform</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">train_transform)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Create the test dataset using ImageFolder</span></span>
<span class="line"><span style="color: #E1E4E8">test_dataset </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> ImageFolder(</span><span style="color: #FFAB70">root</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">data_dir </span><span style="color: #F97583">/</span><span style="color: #E1E4E8"> </span><span style="color: #9ECBFF">"test"</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">transform</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">test_transform)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Create the training dataloader using DataLoader</span></span>
<span class="line"><span style="color: #E1E4E8">training_dataloader </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> DataLoader(</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">dataset</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">training_dataset,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">shuffle</span><span style="color: #F97583">=</span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">batch_size</span><span style="color: #F97583">=</span><span style="color: #79B8FF">BATCH_SIZE</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">num_workers</span><span style="color: #F97583">=</span><span style="color: #79B8FF">2</span></span>
<span class="line"><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Create the test dataloader using DataLoader</span></span>
<span class="line"><span style="color: #E1E4E8">test_dataloader </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> DataLoader(</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">dataset</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">test_dataset,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">shuffle</span><span style="color: #F97583">=</span><span style="color: #79B8FF">False</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">batch_size</span><span style="color: #F97583">=</span><span style="color: #79B8FF">BATCH_SIZE</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #FFAB70">num_workers</span><span style="color: #F97583">=</span><span style="color: #79B8FF">2</span></span>
<span class="line"><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"></span></code></pre>
<p class="z-0">We can visualize a few training dataset images and see their labels</p>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">import</span><span style="color: #E1E4E8"> matplotlib.pyplot </span><span style="color: #F97583">as</span><span style="color: #E1E4E8"> plt</span></span>
<span class="line"><span style="color: #F97583">import</span><span style="color: #E1E4E8"> random</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">num_rows </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">5</span></span>
<span class="line"><span style="color: #E1E4E8">num_cols </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> num_rows</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Create a figure with subplots</span></span>
<span class="line"><span style="color: #E1E4E8">fig, axs </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> plt.subplots(num_rows, num_cols, </span><span style="color: #FFAB70">figsize</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">(</span><span style="color: #79B8FF">10</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">10</span><span style="color: #E1E4E8">))</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Iterate over the subplots and display random images from the training dataset</span></span>
<span class="line"><span style="color: #F97583">for</span><span style="color: #E1E4E8"> i </span><span style="color: #F97583">in</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">range</span><span style="color: #E1E4E8">(num_rows):</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #F97583">for</span><span style="color: #E1E4E8"> j </span><span style="color: #F97583">in</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">range</span><span style="color: #E1E4E8">(num_cols):</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #6A737D"># Choose a random index from the training dataset</span></span>
<span class="line"><span style="color: #E1E4E8">        image_index </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> random.randrange(</span><span style="color: #79B8FF">len</span><span style="color: #E1E4E8">(training_dataset))</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #6A737D"># Display the image in the subplot</span></span>
<span class="line"><span style="color: #E1E4E8">        axs[i, j].imshow(training_dataset[image_index][</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">].permute((</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">2</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">)))</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #6A737D"># Set the title of the subplot as the corresponding class name</span></span>
<span class="line"><span style="color: #E1E4E8">        axs[i, j].set_title(training_dataset.classes[training_dataset[image_index][</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">]], </span><span style="color: #FFAB70">color</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"white"</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #6A737D"># Disable the axis for better visualization</span></span>
<span class="line"><span style="color: #E1E4E8">        axs[i, j].axis(</span><span style="color: #79B8FF">False</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Set the super title of the figure</span></span>
<span class="line"><span style="color: #E1E4E8">fig.suptitle(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">"Random </span><span style="color: #79B8FF">{</span><span style="color: #E1E4E8">num_rows </span><span style="color: #F97583">*</span><span style="color: #E1E4E8"> num_cols</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> images from the training dataset"</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">fontsize</span><span style="color: #F97583">=</span><span style="color: #79B8FF">16</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">color</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"white"</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Set the background color of the figure as black</span></span>
<span class="line"><span style="color: #E1E4E8">fig.set_facecolor(</span><span style="color: #FFAB70">color</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">'black'</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Display the plot</span></span>
<span class="line"><span style="color: #E1E4E8">plt.show()</span></span>
<span class="line"></span></code></pre>
<p class="z-0"><img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/vision_transformer_10_0_sklb3w.png" alt="dataset images" class="rounded-md"></p>
<hr>
<h1>Understanding Vision Transformer Architecture</h1>
<p class="z-0">Let us take some time now to understand the Vision Transformer Architecture. This is the link to the original vision transformer paper: <a href="https://arxiv.org/abs/2010.11929" rel="nofollow">https://arxiv.org/abs/2010.11929</a>.</p>
<p class="z-0">Below, you can see the architecture that is proposed in the image.</p>
<img height="430" src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/Vision_Transformer_Architecture_woi9aw.png">
<p class="z-0">The Vision Transformer (ViT) is a type of Transformer architecture designed for image processing tasks. Unlike traditional Transformers that operate on sequences of word embeddings, ViT operates on sequences of image embeddings. In other words, it breaks down an input image into patches and treats them as a sequence of learnable embeddings.</p>
<p class="z-0">At a broad level, what ViT does is, it:</p>
<ol class="list-outside"><li><strong>Creates Patch Embeddings</strong>
<img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/equations_avmfr1.png" alt="Equations" class="rounded-md"></li>
<li>Passes embeddings through <strong>Transformer Blocks</strong>:</li></ol>
<ul class="list-disc list-inside ml-5 "><li>The patch embeddings, along with the classification token, are passed through multiple Transformer blocks.</li>
<li>Each Transformer block consists of a MultiHead Self-Attention Block (MSA Block) and a Multi-Layer Perceptron Block (MLP Block).</li>
<li>Skip connections are established between the input to the Transformer block and the input to the MSA block, as well as between the input to the MLP block and the output of the MLP block. These skip connections help mitigate the vanishing gradient problem as more Transformer blocks are added.</li></ul>
<ol start="3" class="list-outside"><li>Performs <strong>Classification</strong>:</li></ol>
<ul class="list-disc list-inside ml-5 "><li>The final output from the Transformer blocks is passed through an MLP block.</li>
<li>The classification token, which contains information about the input imageâ€™s class, is used to make predictions.</li></ul>
<p class="z-0">We will dive into each of these steps in detail, starting with the crucial process of creating patch embeddings.</p>
<hr>
<h2>Step 4: Create Patch Embedding Layer</h2>
<p class="z-0">For the ViT paper, we need to perform the following functions on the image before passing to the MultiHead Self Attention Transformer Layer</p>
<ol class="list-outside"><li>Convert the image into patches of 16 x 16 size.</li>
<li>Embed each patch into 768 dimensions. So each patch becomes a <code>[1 x 768] </code> Vector. There will be <span class="math math-inline"><!-- HTML_TAG_START --><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mi>H</mi><mo>Ã—</mo><mi>W</mi><mi mathvariant="normal">/</mi><msup><mi>P</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">N = H \times W / P^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord">/</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><!-- HTML_TAG_END --></span> number of patches for each image. This results in an image that is of the shape <code>[14 x 14 x 768]</code></li>
<li>Flatten the image along a single vector. This will give a <code>[196 x 768]</code>  Matrix, which is our Image Embedding Sequence.</li>
<li>Prepend the Class Token Embeddings to the above output</li>
<li>Add the Position Embeddings to the Class Token and Image Embeddings.</li></ol>
<img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/vision_transformer_process_neehov.gif">
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #79B8FF">PATCH_SIZE</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">16</span></span>
<span class="line"><span style="color: #79B8FF">IMAGE_WIDTH</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">224</span></span>
<span class="line"><span style="color: #79B8FF">IMAGE_HEIGHT</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">IMAGE_WIDTH</span></span>
<span class="line"><span style="color: #79B8FF">IMAGE_CHANNELS</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">3</span></span>
<span class="line"><span style="color: #79B8FF">EMBEDDING_DIMS</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">IMAGE_CHANNELS</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">*</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">PATCH_SIZE</span><span style="color: #F97583">**</span><span style="color: #79B8FF">2</span></span>
<span class="line"><span style="color: #79B8FF">NUM_OF_PATCHES</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">int</span><span style="color: #E1E4E8">((</span><span style="color: #79B8FF">IMAGE_WIDTH</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">*</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">IMAGE_HEIGHT</span><span style="color: #E1E4E8">) </span><span style="color: #F97583">/</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">PATCH_SIZE</span><span style="color: #F97583">**</span><span style="color: #79B8FF">2</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D">#the image width and image height should be divisible by patch size. This is a check to see that.</span></span>
<span class="line"></span>
<span class="line"><span style="color: #F97583">assert</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">IMAGE_WIDTH</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">%</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">PATCH_SIZE</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">==</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">and</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">IMAGE_HEIGHT</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">%</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">PATCH_SIZE</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">==</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8"> , </span><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #9ECBFF">"Image Width is not divisible by patch size"</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"></span></code></pre>
<h2>Step 4.1</h2>
<p class="z-0">Converting the image into patches of 16 x 16 and creating an embedding vector for each patch of size 768.</p>
<p class="z-0">This can be accomplished by using a Conv2D Layer with a kernel_size equal to patch_size and a stride equal to patch_size</p>
<p class="z-0"><img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/CNN_Flatten_vfq1q6.png" alt="Convolution plus Flatten" class="rounded-md"></p>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">conv_layer </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.Conv2d(</span><span style="color: #FFAB70">in_channels</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">IMAGE_CHANNELS</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">out_channels</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">EMBEDDING_DIMS</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">kernel_size</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">PATCH_SIZE</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">stride</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">PATCH_SIZE</span><span style="color: #E1E4E8">)</span></span></code></pre>
<p class="z-0">We can pass a random image into the convolutional layer and see what happens</p>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">random_images, random_labels </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">next</span><span style="color: #E1E4E8">(</span><span style="color: #79B8FF">iter</span><span style="color: #E1E4E8">(training_dataloader))</span></span>
<span class="line"><span style="color: #E1E4E8">random_image </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> random_images[</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">]</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Create a new figure</span></span>
<span class="line"><span style="color: #E1E4E8">fig </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> plt.figure(</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Display the random image</span></span>
<span class="line"><span style="color: #E1E4E8">plt.imshow(random_image.permute((</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">2</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">)))</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Disable the axis for better visualization</span></span>
<span class="line"><span style="color: #E1E4E8">plt.axis(</span><span style="color: #79B8FF">False</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Set the title of the image</span></span>
<span class="line"><span style="color: #E1E4E8">plt.title(training_dataset.classes[random_labels[</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">]], </span><span style="color: #FFAB70">color</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"white"</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Set the background color of the figure as black</span></span>
<span class="line"><span style="color: #E1E4E8">fig.set_facecolor(</span><span style="color: #FFAB70">color</span><span style="color: #F97583">=</span><span style="color: #9ECBFF">"black"</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<p class="z-0"><img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/vision_transformer_17_0_ckmhgs.png" alt="random image" class="rounded-md"></p>
<p class="z-0">We need to change the shape to <code>[1, 14, 14, 768]</code> and flatten the output to <code>[1, 196, 768]</code></p>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #6A737D"># Pass the image through the convolution layer</span></span>
<span class="line"><span style="color: #E1E4E8">image_through_conv </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> conv_layer(random_image.unsqueeze(</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">))</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">'Shape of embeddings through the conv layer -&gt; </span><span style="color: #79B8FF">{list</span><span style="color: #E1E4E8">(image_through_conv.shape)</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> &lt;- [batch_size, num_of_patch_rows,num_patch_cols embedding_dims]'</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Permute the dimensions of image_through_conv to match the expected shape</span></span>
<span class="line"><span style="color: #E1E4E8">image_through_conv </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> image_through_conv.permute((</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">2</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">3</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">))</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Create a flatten layer using nn.Flatten</span></span>
<span class="line"><span style="color: #E1E4E8">flatten_layer </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.Flatten(</span><span style="color: #FFAB70">start_dim</span><span style="color: #F97583">=</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">end_dim</span><span style="color: #F97583">=</span><span style="color: #79B8FF">2</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Pass the image_through_conv through the flatten layer</span></span>
<span class="line"><span style="color: #E1E4E8">image_through_conv_and_flatten </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> flatten_layer(image_through_conv)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Print the shape of the embedded image</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">'Shape of embeddings through the flatten layer -&gt; </span><span style="color: #79B8FF">{list</span><span style="color: #E1E4E8">(image_through_conv_and_flatten.shape)</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> &lt;- [batch_size, num_of_patches, embedding_dims]'</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #6A737D"># Assign the embedded image to a variable</span></span>
<span class="line"><span style="color: #E1E4E8">embedded_image </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> image_through_conv_and_flatten</span></span>
<span class="line"></span></code></pre>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">Shape of embeddings through the conv layer -&gt; [1, 768, 14, 14] &lt;- [batch_size, num_of_patch_rows,num_patch_cols embedding_dims]</span></span>
<span class="line"><span style="color: #E1E4E8">Shape of embeddings through the flatten layer -&gt; [1, 196, 768] &lt;- [batch_size, num_of_patches, embedding_dims]</span></span></code></pre>
<h2>4.2. Prepending the Class Token Embedding and Adding the Position Embeddings</h2>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">class_token_embeddings </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.Parameter(torch.rand((</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">,</span><span style="color: #79B8FF">EMBEDDING_DIMS</span><span style="color: #E1E4E8">), </span><span style="color: #FFAB70">requires_grad</span><span style="color: #E1E4E8">  </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">))</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">'Shape of class_token_embeddings --&gt; </span><span style="color: #79B8FF">{list</span><span style="color: #E1E4E8">(class_token_embeddings.shape)</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> &lt;-- [batch_size, 1, emdedding_dims]'</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">embedded_image_with_class_token_embeddings </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> torch.cat((class_token_embeddings, embedded_image), </span><span style="color: #FFAB70">dim</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">'</span><span style="color: #79B8FF">\n</span><span style="color: #9ECBFF">Shape of image embeddings with class_token_embeddings --&gt; </span><span style="color: #79B8FF">{list</span><span style="color: #E1E4E8">(embedded_image_with_class_token_embeddings.shape)</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> &lt;-- [batch_size, num_of_patches+1, embeddiing_dims]'</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">position_embeddings </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.Parameter(torch.rand((</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">NUM_OF_PATCHES</span><span style="color: #F97583">+</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">EMBEDDING_DIMS</span><span style="color: #E1E4E8"> ), </span><span style="color: #FFAB70">requires_grad</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8"> ))</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">'</span><span style="color: #79B8FF">\n</span><span style="color: #9ECBFF">Shape of position_embeddings --&gt; </span><span style="color: #79B8FF">{list</span><span style="color: #E1E4E8">(position_embeddings.shape)</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> &lt;-- [batch_size, num_patches+1, embeddings_dims]'</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">final_embeddings </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedded_image_with_class_token_embeddings </span><span style="color: #F97583">+</span><span style="color: #E1E4E8"> position_embeddings</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">'</span><span style="color: #79B8FF">\n</span><span style="color: #9ECBFF">Shape of final_embeddings --&gt; </span><span style="color: #79B8FF">{list</span><span style="color: #E1E4E8">(final_embeddings.shape)</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> &lt;-- [batch_size, num_patches+1, embeddings_dims]'</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">Shape of </span><span style="color: #79B8FF">`class_token_embeddings`</span><span style="color: #E1E4E8"> --&gt; </span><span style="color: #79B8FF">`[1, 1, 768]`</span><span style="color: #E1E4E8"> &lt;-- </span><span style="color: #79B8FF">`[batch_size, 1, emdedding_dims]`</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">Shape of </span><span style="color: #79B8FF">`image embeddings with class_token_embeddings`</span><span style="color: #E1E4E8"> --&gt; </span><span style="color: #79B8FF">`[1, 197, 768]`</span><span style="color: #E1E4E8"> &lt;-- </span><span style="color: #79B8FF">`[batch_size, num_of_patches+1, embeddiing_dims]`</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">Shape of </span><span style="color: #79B8FF">`position_embeddings`</span><span style="color: #E1E4E8"> --&gt; </span><span style="color: #79B8FF">`[1, 197, 768]`</span><span style="color: #E1E4E8"> &lt;-- </span><span style="color: #79B8FF">`[batch_size, num_patches+1, embeddings_dims]`</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">Shape of </span><span style="color: #79B8FF">`final_embeddings`</span><span style="color: #E1E4E8"> --&gt; </span><span style="color: #79B8FF">`[1, 197, 768]`</span><span style="color: #E1E4E8"> &lt;-- </span><span style="color: #79B8FF">`[batch_size, num_patches+1, embeddings_dims]`</span></span></code></pre>
<h2>Put the PatchEmbedddingLayer Together</h2>
<p class="z-0">We will inherit from the PyTorch <code>nn.Module</code> to create our custom layer which takes in an image and throws out the patch embeddings which consists of the Image Embeddings, Class Token Embeddings and the Position Embeddings.</p>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">class</span><span style="color: #E1E4E8"> </span><span style="color: #B392F0">PatchEmbeddingLayer</span><span style="color: #E1E4E8">(</span><span style="color: #B392F0">nn</span><span style="color: #E1E4E8">.</span><span style="color: #B392F0">Module</span><span style="color: #E1E4E8">):</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #F97583">def</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">__init__</span><span style="color: #E1E4E8">(self, in_channels, patch_size, embedding_dim,):</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #79B8FF">super</span><span style="color: #E1E4E8">().</span><span style="color: #79B8FF">__init__</span><span style="color: #E1E4E8">()</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.patch_size </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> patch_size</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.embedding_dim </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dim</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.in_channels </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> in_channels</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.conv_layer </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.Conv2d(</span><span style="color: #FFAB70">in_channels</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">in_channels, </span><span style="color: #FFAB70">out_channels</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">embedding_dim, </span><span style="color: #FFAB70">kernel_size</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">patch_size, </span><span style="color: #FFAB70">stride</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">patch_size)</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.flatten_layer </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.Flatten(</span><span style="color: #FFAB70">start_dim</span><span style="color: #F97583">=</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">end_dim</span><span style="color: #F97583">=</span><span style="color: #79B8FF">2</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.class_token_embeddings </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.Parameter(torch.rand((</span><span style="color: #79B8FF">BATCH_SIZE</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">EMBEDDING_DIMS</span><span style="color: #E1E4E8">), </span><span style="color: #FFAB70">requires_grad</span><span style="color: #F97583">=</span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">))</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.position_embeddings </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.Parameter(torch.rand((</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">NUM_OF_PATCHES</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">+</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">EMBEDDING_DIMS</span><span style="color: #E1E4E8">), </span><span style="color: #FFAB70">requires_grad</span><span style="color: #F97583">=</span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">))</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #F97583">def</span><span style="color: #E1E4E8"> </span><span style="color: #B392F0">forward</span><span style="color: #E1E4E8">(self, x):</span></span>
<span class="line"><span style="color: #E1E4E8">        output </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> torch.cat((</span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.class_token_embeddings, </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.flatten_layer(</span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.conv_layer(x).permute((</span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">2</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">3</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">)))), </span><span style="color: #FFAB70">dim</span><span style="color: #F97583">=</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">) </span><span style="color: #F97583">+</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.position_embeddings</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #F97583">return</span><span style="color: #E1E4E8"> output</span></span>
<span class="line"></span></code></pre>
<p class="z-0">Letâ€™s pass a batch of random images from our patch embedding layer. </p>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">patch_embedding_layer </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> PatchEmbeddingLayer(</span><span style="color: #FFAB70">in_channels</span><span style="color: #F97583">=</span><span style="color: #79B8FF">IMAGE_CHANNELS</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">patch_size</span><span style="color: #F97583">=</span><span style="color: #79B8FF">PATCH_SIZE</span><span style="color: #E1E4E8">, </span><span style="color: #FFAB70">embedding_dim</span><span style="color: #F97583">=</span><span style="color: #79B8FF">IMAGE_CHANNELS</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">*</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">PATCH_SIZE</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">**</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">2</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">patch_embeddings </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> patch_embedding_layer(random_images)</span></span>
<span class="line"><span style="color: #E1E4E8">patch_embeddings.shape</span></span>
<span class="line"></span></code></pre>
<p class="z-0">torch.Size([32, 197, 768])</p>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">summary(</span><span style="color: #FFAB70">model</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">patch_embedding_layer,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">input_size</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">(</span><span style="color: #79B8FF">BATCH_SIZE</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">3</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">224</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">224</span><span style="color: #E1E4E8">), </span><span style="color: #6A737D"># (batch_size, input_channels, img_width, img_height)</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">col_names</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">[</span><span style="color: #9ECBFF">"input_size"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"output_size"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"num_params"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"trainable"</span><span style="color: #E1E4E8">],</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">col_width</span><span style="color: #F97583">=</span><span style="color: #79B8FF">20</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">row_settings</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">[</span><span style="color: #9ECBFF">"var_names"</span><span style="color: #E1E4E8">])</span></span></code></pre>
<p class="z-0"><img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/patch_embedding_layer_v5gefy.png" alt="Patch Embedding" class="rounded-md"></p>
<hr>
<h2>Step 5. Creating the Multi-Head Self Attention (MSA) Block.</h2>
<h3>Understanding MSA Block</h3>
<p class="z-0">As a first step of putting together our transformer block for the Vision Transformer model, we will be creating a MultiHead Self Attention Block.</p>
<p class="z-0">Let us take a moment to understand the MSA Block. The MSA block itself consists of a LayerNorm layer and the Multi-Head Attention Layer. The layernorm layer essentially normalizes our patch embeddings data across the embeddings dimension. The Multi-Head Attention layer takes in the input data as 3 form of learnable vectors namely <strong>query</strong>, <strong>key</strong> and <strong>value</strong>, collectively known as <strong>qkv</strong> vectors. These vectors together form the relationship between each patch of the input sequence with every other patch in the same sequence (hence the name <strong>self-attention</strong>).</p>
<p class="z-0">So, our input shape to the MSA Block will be the shape of our patch embeddings that we made using the PatchEmbeddingLayer -&gt; <code>[batch_size, sequence_length, embedding_dimensions]</code>. And the output from the MSA layer will be of the same shape as the input.</p>
<h3>MSA Block Code</h3>
<p class="z-0">Now let us begin to write code for out MSA Block. This will be short as PyTorch has a pre-built implementation of <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html" rel="nofollow">LayerNorm</a> and the <a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html" rel="nofollow">MultiHeadAttention Layer</a>. We just have to pass the right arguments to suit our architecture. We can find the various parameters that are required for out MSA block in this table from the original ViT Paper.</p>
<img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/ViT-model-variants_gfeotm.png">
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">class</span><span style="color: #E1E4E8"> </span><span style="color: #B392F0">MultiHeadSelfAttentionBlock</span><span style="color: #E1E4E8">(</span><span style="color: #B392F0">nn</span><span style="color: #E1E4E8">.</span><span style="color: #B392F0">Module</span><span style="color: #E1E4E8">):</span></span>
<span class="line"><span style="color: #E1E4E8">  </span><span style="color: #F97583">def</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">__init__</span><span style="color: #E1E4E8">(self,</span></span>
<span class="line"><span style="color: #E1E4E8">               embedding_dims </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">768</span><span style="color: #E1E4E8">, </span><span style="color: #6A737D"># Hidden Size D in the ViT Paper Table 1</span></span>
<span class="line"><span style="color: #E1E4E8">               num_heads </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">12</span><span style="color: #E1E4E8">,  </span><span style="color: #6A737D"># Heads in the ViT Paper Table 1</span></span>
<span class="line"><span style="color: #E1E4E8">               attn_dropout </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">0.0</span><span style="color: #E1E4E8"> </span><span style="color: #6A737D"># Default to Zero as there is no dropout for the the MSA Block as per the ViT Paper</span></span>
<span class="line"><span style="color: #E1E4E8">               ):</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">super</span><span style="color: #E1E4E8">().</span><span style="color: #79B8FF">__init__</span><span style="color: #E1E4E8">()</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.embedding_dims </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.num_head </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> num_heads</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.attn_dropout </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> attn_dropout</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.layernorm </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.LayerNorm(</span><span style="color: #FFAB70">normalized_shape</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.multiheadattention </span><span style="color: #F97583">=</span><span style="color: #E1E4E8">  nn.MultiheadAttention(</span><span style="color: #FFAB70">num_heads</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> num_heads,</span></span>
<span class="line"><span style="color: #E1E4E8">                                                     </span><span style="color: #FFAB70">embed_dim</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims,</span></span>
<span class="line"><span style="color: #E1E4E8">                                                     </span><span style="color: #FFAB70">dropout</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> attn_dropout,</span></span>
<span class="line"><span style="color: #E1E4E8">                                                     </span><span style="color: #FFAB70">batch_first</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">True</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">                                                    )</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">  </span><span style="color: #F97583">def</span><span style="color: #E1E4E8"> </span><span style="color: #B392F0">forward</span><span style="color: #E1E4E8">(self, x):</span></span>
<span class="line"><span style="color: #E1E4E8">    x </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.layernorm(x)</span></span>
<span class="line"><span style="color: #E1E4E8">    output,_ </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.multiheadattention(</span><span style="color: #FFAB70">query</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">x, </span><span style="color: #FFAB70">key</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">x, </span><span style="color: #FFAB70">value</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">x,</span><span style="color: #FFAB70">need_weights</span><span style="color: #F97583">=</span><span style="color: #79B8FF">False</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #F97583">return</span><span style="color: #E1E4E8"> output</span></span></code></pre>
<h3>Letâ€™s test our MSA Block</h3>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">multihead_self_attention_block </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> MultiHeadSelfAttentionBlock(</span><span style="color: #FFAB70">embedding_dims</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">EMBEDDING_DIMS</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">                                                             </span><span style="color: #FFAB70">num_heads</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">12</span></span>
<span class="line"><span style="color: #E1E4E8">                                                             )</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">'Shape of the input Patch Embeddings =&gt; </span><span style="color: #79B8FF">{list</span><span style="color: #E1E4E8">(patch_embeddings.shape)</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> &lt;= [batch_size, num_patches+1, embedding_dims ]'</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">'Shape of the output from MSA Block =&gt; </span><span style="color: #79B8FF">{list</span><span style="color: #E1E4E8">(multihead_self_attention_block(patch_embeddings).shape)</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> &lt;= [batch_size, num_patches+1, embedding_dims ]'</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">Shape of the input Patch Embeddings =&gt; [32, 197, 768] &lt;= [batch_size, num_patches+1, embedding_dims ]</span></span>
<span class="line"><span style="color: #E1E4E8">Shape of the output from MSA Block =&gt; [32, 197, 768] &lt;= [batch_size, num_patches+1, embedding_dims ]</span></span></code></pre>
<p class="z-0">Beautiful, so seems like our MSA block is working. We can get more information about the MSA Block using torchinfo</p>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">summary(</span><span style="color: #FFAB70">model</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">multihead_self_attention_block,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">input_size</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">(</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">197</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">768</span><span style="color: #E1E4E8">), </span><span style="color: #6A737D"># (batch_size, num_patches, embedding_dimension)</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">col_names</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">[</span><span style="color: #9ECBFF">"input_size"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"output_size"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"num_params"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"trainable"</span><span style="color: #E1E4E8">],</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">col_width</span><span style="color: #F97583">=</span><span style="color: #79B8FF">20</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">row_settings</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">[</span><span style="color: #9ECBFF">"var_names"</span><span style="color: #E1E4E8">])</span></span>
<span class="line"></span></code></pre>
<p class="z-0"><img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/msa_block_pudxdm.png" alt="MSA_Block" class="rounded-md"></p>
<hr>
<h2>Step 6. Creating the Machine Learning Perceptron (MLP) Block</h2>
<h3>Understanding the MLP Block</h3>
<p class="z-0">The Machine Learning Perceptron (MLP) Block in the transformer is a combination of a Fully Connected Layer (also called as a Linear Layer or a Dense Layer) and a non-linear layer. In the case of ViT, the non-linear layer is a GeLU layer.</p>
<img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/Screen_Shot_2020-05-27_at_12.48.44_PM.png" width="300">
<p class="z-0">The transformer also implements a Dropout layer to reduce overfitting. So the MLP Block will look something like this:</p>
<p class="z-0">Input â†’ Linear â†’ GeLU â†’ Dropout â†’ Linear â†’ Dropout</p>
<p class="z-0">According to the paper, the first Linear Layer scales the embedding dimensions to the 3072 dimensions (for the ViT-16/Base). The Dropout is set to 0.1 and the second Linear Layer scales down the dimensions back to the embedding dimensions.</p>
<h3>MLP Block Code</h3>
<p class="z-0">Let us now assemble our MLP Block. According to the ViT Paper, the output from the MSA Block added to the input to the MSA Block (denoted by the skip/residual connection in the model architecture figure) is passed as input to the MLP Block. All the layers are provided by the PyTorch library. We just need to assemble it.</p>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">class</span><span style="color: #E1E4E8"> </span><span style="color: #B392F0">MachineLearningPerceptronBlock</span><span style="color: #E1E4E8">(</span><span style="color: #B392F0">nn</span><span style="color: #E1E4E8">.</span><span style="color: #B392F0">Module</span><span style="color: #E1E4E8">):</span></span>
<span class="line"><span style="color: #E1E4E8">  </span><span style="color: #F97583">def</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">__init__</span><span style="color: #E1E4E8">(self, embedding_dims, mlp_size, mlp_dropout):</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">super</span><span style="color: #E1E4E8">().</span><span style="color: #79B8FF">__init__</span><span style="color: #E1E4E8">()</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.embedding_dims </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.mlp_size </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> mlp_size</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.dropout </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> mlp_dropout</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.layernorm </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.LayerNorm(</span><span style="color: #FFAB70">normalized_shape</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims)</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.mlp </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.Sequential(</span></span>
<span class="line"><span style="color: #E1E4E8">        nn.Linear(</span><span style="color: #FFAB70">in_features</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims, </span><span style="color: #FFAB70">out_features</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> mlp_size),</span></span>
<span class="line"><span style="color: #E1E4E8">        nn.GELU(),</span></span>
<span class="line"><span style="color: #E1E4E8">        nn.Dropout(</span><span style="color: #FFAB70">p</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> mlp_dropout),</span></span>
<span class="line"><span style="color: #E1E4E8">        nn.Linear(</span><span style="color: #FFAB70">in_features</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> mlp_size, </span><span style="color: #FFAB70">out_features</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims),</span></span>
<span class="line"><span style="color: #E1E4E8">        nn.Dropout(</span><span style="color: #FFAB70">p</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> mlp_dropout)</span></span>
<span class="line"><span style="color: #E1E4E8">    )</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">  </span><span style="color: #F97583">def</span><span style="color: #E1E4E8"> </span><span style="color: #B392F0">forward</span><span style="color: #E1E4E8">(self, x):</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #F97583">return</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.mlp(</span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.layernorm(x))</span></span></code></pre>
<h3>Letâ€™s test our MLP Block</h3>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">mlp_block </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> MachineLearningPerceptronBlock(</span><span style="color: #FFAB70">embedding_dims</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">EMBEDDING_DIMS</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">                                           </span><span style="color: #FFAB70">mlp_size</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">3072</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">                                           </span><span style="color: #FFAB70">mlp_dropout</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">0.1</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">summary(</span><span style="color: #FFAB70">model</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">mlp_block,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">input_size</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">(</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">197</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">768</span><span style="color: #E1E4E8">), </span><span style="color: #6A737D"># (batch_size, num_patches, embedding_dimension)</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">col_names</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">[</span><span style="color: #9ECBFF">"input_size"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"output_size"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"num_params"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"trainable"</span><span style="color: #E1E4E8">],</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">col_width</span><span style="color: #F97583">=</span><span style="color: #79B8FF">20</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">row_settings</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">[</span><span style="color: #9ECBFF">"var_names"</span><span style="color: #E1E4E8">])</span></span>
<span class="line"></span></code></pre>
<p class="z-0"><img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/mlp_block_gm2fju.png" alt="MLP_Block" class="rounded-md"></p>
<p class="z-0">Amazing, looks like the MLP Block is also working as expected.</p>
<hr>
<h2>Step 7. Putting together the Transformer Block</h2>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">class</span><span style="color: #E1E4E8"> </span><span style="color: #B392F0">TransformerBlock</span><span style="color: #E1E4E8">(</span><span style="color: #B392F0">nn</span><span style="color: #E1E4E8">.</span><span style="color: #B392F0">Module</span><span style="color: #E1E4E8">):</span></span>
<span class="line"><span style="color: #E1E4E8">  </span><span style="color: #F97583">def</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">__init__</span><span style="color: #E1E4E8">(self, embedding_dims </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">768</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               mlp_dropout</span><span style="color: #F97583">=</span><span style="color: #79B8FF">0.1</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               attn_dropout</span><span style="color: #F97583">=</span><span style="color: #79B8FF">0.0</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               mlp_size </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">3072</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               num_heads </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">12</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               ):</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">super</span><span style="color: #E1E4E8">().</span><span style="color: #79B8FF">__init__</span><span style="color: #E1E4E8">()</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.msa_block </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> MultiHeadSelfAttentionBlock(</span><span style="color: #FFAB70">embedding_dims</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims,</span></span>
<span class="line"><span style="color: #E1E4E8">                                                 </span><span style="color: #FFAB70">num_heads</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> num_heads,</span></span>
<span class="line"><span style="color: #E1E4E8">                                                 </span><span style="color: #FFAB70">attn_dropout</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> attn_dropout)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.mlp_block </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> MachineLearningPerceptronBlock(</span><span style="color: #FFAB70">embedding_dims</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims,</span></span>
<span class="line"><span style="color: #E1E4E8">                                                    </span><span style="color: #FFAB70">mlp_size</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> mlp_size,</span></span>
<span class="line"><span style="color: #E1E4E8">                                                    </span><span style="color: #FFAB70">mlp_dropout</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> mlp_dropout,</span></span>
<span class="line"><span style="color: #E1E4E8">                                                    )</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">  </span><span style="color: #F97583">def</span><span style="color: #E1E4E8"> </span><span style="color: #B392F0">forward</span><span style="color: #E1E4E8">(self,x):</span></span>
<span class="line"><span style="color: #E1E4E8">    x </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.msa_block(x) </span><span style="color: #F97583">+</span><span style="color: #E1E4E8"> x</span></span>
<span class="line"><span style="color: #E1E4E8">    x </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.mlp_block(x) </span><span style="color: #F97583">+</span><span style="color: #E1E4E8"> x</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #F97583">return</span><span style="color: #E1E4E8"> x</span></span>
<span class="line"></span></code></pre>
<h2>Testing the Transformer Block</h2>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">transformer_block </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> TransformerBlock(</span><span style="color: #FFAB70">embedding_dims</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">EMBEDDING_DIMS</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">                                     </span><span style="color: #FFAB70">mlp_dropout</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">0.1</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">                                     </span><span style="color: #FFAB70">attn_dropout</span><span style="color: #F97583">=</span><span style="color: #79B8FF">0.0</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">                                     </span><span style="color: #FFAB70">mlp_size</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">3072</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">                                     </span><span style="color: #FFAB70">num_heads</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">12</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">'Shape of the input Patch Embeddings =&gt; </span><span style="color: #79B8FF">{list</span><span style="color: #E1E4E8">(patch_embeddings.shape)</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> &lt;= [batch_size, num_patches+1, embedding_dims ]'</span><span style="color: #E1E4E8">)</span></span>
<span class="line"><span style="color: #79B8FF">print</span><span style="color: #E1E4E8">(</span><span style="color: #F97583">f</span><span style="color: #9ECBFF">'Shape of the output from Transformer Block =&gt; </span><span style="color: #79B8FF">{list</span><span style="color: #E1E4E8">(transformer_block(patch_embeddings).shape)</span><span style="color: #79B8FF">}</span><span style="color: #9ECBFF"> &lt;= [batch_size, num_patches+1, embedding_dims ]'</span><span style="color: #E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">Shape of the input Patch Embeddings =&gt; [32, 197, 768] &lt;= [batch_size, num_patches+1, embedding_dims ]</span></span>
<span class="line"><span style="color: #E1E4E8">Shape of the output from Transformer Block =&gt; [32, 197, 768] &lt;= [batch_size, num_patches+1, embedding_dims ]</span></span></code></pre>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">summary(</span><span style="color: #FFAB70">model</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">transformer_block,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">input_size</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">(</span><span style="color: #79B8FF">1</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">197</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">768</span><span style="color: #E1E4E8">), </span><span style="color: #6A737D"># (batch_size, num_patches, embedding_dimension)</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">col_names</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">[</span><span style="color: #9ECBFF">"input_size"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"output_size"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"num_params"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"trainable"</span><span style="color: #E1E4E8">],</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">col_width</span><span style="color: #F97583">=</span><span style="color: #79B8FF">20</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">row_settings</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">[</span><span style="color: #9ECBFF">"var_names"</span><span style="color: #E1E4E8">])</span></span></code></pre>
<p class="z-0"><img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/transformer_block_kq7xx1.png" alt="Transformer Block" class="rounded-md"></p>
<hr>
<h2>Step 8. Creating the ViT Model</h2>
<p class="z-0">Finally, letâ€™s put together our ViT Model. Itâ€™s going to be as simple as combining whatever we have done till now. One slight addition will be the classifier layer that we will add. In ViT the classifier layer is a simple Linear layer with Layer Normalization. The classification is performed on the zeroth index of the output of the transformer.</p>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #F97583">class</span><span style="color: #E1E4E8"> </span><span style="color: #B392F0">ViT</span><span style="color: #E1E4E8">(</span><span style="color: #B392F0">nn</span><span style="color: #E1E4E8">.</span><span style="color: #B392F0">Module</span><span style="color: #E1E4E8">):</span></span>
<span class="line"><span style="color: #E1E4E8">  </span><span style="color: #F97583">def</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">__init__</span><span style="color: #E1E4E8">(self, img_size </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">224</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               in_channels </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">3</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               patch_size </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">16</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               embedding_dims </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">768</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               num_transformer_layers </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">12</span><span style="color: #E1E4E8">, </span><span style="color: #6A737D"># from table 1 above</span></span>
<span class="line"><span style="color: #E1E4E8">               mlp_dropout </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">0.1</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               attn_dropout </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">0.0</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               mlp_size </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">3072</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               num_heads </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">12</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">               num_classes </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">1000</span><span style="color: #E1E4E8">):</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">super</span><span style="color: #E1E4E8">().</span><span style="color: #79B8FF">__init__</span><span style="color: #E1E4E8">()</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.patch_embedding_layer </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> PatchEmbeddingLayer(</span><span style="color: #FFAB70">in_channels</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> in_channels,</span></span>
<span class="line"><span style="color: #E1E4E8">                                                     </span><span style="color: #FFAB70">patch_size</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">patch_size,</span></span>
<span class="line"><span style="color: #E1E4E8">                                                     </span><span style="color: #FFAB70">embedding_dim</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims)</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.transformer_encoder </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.Sequential(</span><span style="color: #F97583">*</span><span style="color: #E1E4E8">[TransformerBlock(</span><span style="color: #FFAB70">embedding_dims</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims,</span></span>
<span class="line"><span style="color: #E1E4E8">                                              </span><span style="color: #FFAB70">mlp_dropout</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> mlp_dropout,</span></span>
<span class="line"><span style="color: #E1E4E8">                                              </span><span style="color: #FFAB70">attn_dropout</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> attn_dropout,</span></span>
<span class="line"><span style="color: #E1E4E8">                                              </span><span style="color: #FFAB70">mlp_size</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> mlp_size,</span></span>
<span class="line"><span style="color: #E1E4E8">                                              </span><span style="color: #FFAB70">num_heads</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> num_heads) </span><span style="color: #F97583">for</span><span style="color: #E1E4E8"> _ </span><span style="color: #F97583">in</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">range</span><span style="color: #E1E4E8">(num_transformer_layers)])</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.classifier </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> nn.Sequential(nn.LayerNorm(</span><span style="color: #FFAB70">normalized_shape</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims),</span></span>
<span class="line"><span style="color: #E1E4E8">                                    nn.Linear(</span><span style="color: #FFAB70">in_features</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> embedding_dims,</span></span>
<span class="line"><span style="color: #E1E4E8">                                              </span><span style="color: #FFAB70">out_features</span><span style="color: #E1E4E8"> </span><span style="color: #F97583">=</span><span style="color: #E1E4E8"> num_classes))</span></span>
<span class="line"></span>
<span class="line"><span style="color: #E1E4E8">  </span><span style="color: #F97583">def</span><span style="color: #E1E4E8"> </span><span style="color: #B392F0">forward</span><span style="color: #E1E4E8">(self, x):</span></span>
<span class="line"><span style="color: #E1E4E8">    </span><span style="color: #F97583">return</span><span style="color: #E1E4E8"> </span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.classifier(</span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.transformer_encoder(</span><span style="color: #79B8FF">self</span><span style="color: #E1E4E8">.patch_embedding_layer(x))[:, </span><span style="color: #79B8FF">0</span><span style="color: #E1E4E8">])</span></span></code></pre>
<pre class="shiki github-dark" style="background-color: #24292e" tabindex="0"><code><span class="line"><span style="color: #E1E4E8">summary(</span><span style="color: #FFAB70">model</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">vit,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">input_size</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">(</span><span style="color: #79B8FF">BATCH_SIZE</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">3</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">224</span><span style="color: #E1E4E8">, </span><span style="color: #79B8FF">224</span><span style="color: #E1E4E8">), </span><span style="color: #6A737D"># (batch_size, num_patches, embedding_dimension)</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">col_names</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">[</span><span style="color: #9ECBFF">"input_size"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"output_size"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"num_params"</span><span style="color: #E1E4E8">, </span><span style="color: #9ECBFF">"trainable"</span><span style="color: #E1E4E8">],</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">col_width</span><span style="color: #F97583">=</span><span style="color: #79B8FF">20</span><span style="color: #E1E4E8">,</span></span>
<span class="line"><span style="color: #E1E4E8">        </span><span style="color: #FFAB70">row_settings</span><span style="color: #F97583">=</span><span style="color: #E1E4E8">[</span><span style="color: #9ECBFF">"var_names"</span><span style="color: #E1E4E8">])</span></span></code></pre>
<p class="z-0"><img src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/vit_model_l7ycte.png" alt="ViT Model" class="rounded-md"></p>
<p class="z-0">Thatâ€™s it. Now you can train this model just like you would any other model in PyTorch. Let me know how it works out for you. I hope this step-by-step guide has helped you understand the Vision Transformer and inspired you to dive deeper into the world of computer vision and transformer models. With the knowledge gained, you are well-equipped to push the boundaries of computer vision and unlock the potential of these groundbreaking architectures.</p>
<p class="z-0">So go ahead, build upon what you have learned, and let your imagination run wild as you leverage the Vision Transformer to tackle exciting visual challenges. Happy coding!</p>
<hr>
<br>
<div style="display: flex; gap:10px; align-items: center"><img width="90" height="90" src="./Building a Vision Transformer from Scratch in PyTorchðŸ”¥_files/profile_lyql45.jpg">
<div style="display: flex; flex-direction:column; gap:10px; justify-content:space-between"><p style="padding:0; margin:0">my website: <a href="http://www.akshaymakes.com/">http://www.akshaymakes.com/</a></p>
<p style="padding:0; margin:0">linkedin: <a href="https://www.linkedin.com/in/akshay-ballal/">https://www.linkedin.com/in/akshay-ballal/</a></p>
<p style="padding:0; margin:0">twitter: <a href="https://twitter.com/akshayballal95">https://twitter.com/akshayballal95/</a></p></div></div></div></div> </main> <footer id="page-footer" class="flex-none "><div class="md:flex-row flex flex-col gap-5 items-center justify-center pt-5 ml-10 mr-10 mb-3 whitespace-nowrap border-t-2"><p class="text-sm text-thin md:grow unstyled md:mr-10">@2023 by Akshay Ballal</p> <div class="md:flex-row flex flex-col md:items-start items-center md:gap-12 gap-5"><div class="flex flex-col justify-center items-center text-sm unstyled"><div class="font-semibold mb-2">Call</div> <div>+31-0645791146</div></div> <div class="flex flex-col justify-center items-center text-sm unstyled"><div class="font-semibold mb-2">Email</div> <div>arballal95@gmail.com</div></div> <div class="flex flex-col justify-center items-center text-sm unstyled"><div class="font-semibold mb-2">Follow</div> <div class="flex gap-3 items-center "><a class="unstyled" href="https://twitter.com/akshayballal95"><i class="fa-brands fa-twitter"></i></a> <a class="unstyled" href="https://www.facebook.com/arballal/"><i class="fa-brands fa-facebook"></i></a> <a class="unstyled" href="https://github.com/akshayballal95"><i class="fa-brands fa-github"></i></a> <a class="unstyled" href="https://www.linkedin.com/in/akshay-ballal/"><i class="fa-brands fa-linkedin"></i></a></div></div></div></div></footer></div> </div> </div>  <div id="svelte-announcer" aria-live="assertive" aria-atomic="true" style="position: absolute; left: 0px; top: 0px; clip: rect(0px, 0px, 0px, 0px); clip-path: inset(50%); overflow: hidden; white-space: nowrap; width: 1px; height: 1px;"></div></div>


<span id="PING_IFRAME_FORM_DETECTION" style="display: none;"></span></body></html>